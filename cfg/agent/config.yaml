seed: -1

clip_observations: 5.0
clip_actions: 1.0

policy: # only works for MlpPolicy right now
  pi_hid_sizes: [512, 512, 64]
  vf_hid_sizes: [512, 512, 64]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
  feature_dim: 128
  pointnet_sa_module:
    group_all: true
    mlp:
    - 256
    - 512
    - 1024
  topk_confidence: 1024  #200
  training_env_num: 20
  valitating_env_num: 5
  num_env: 16      #每个step多少环境  20
  batch_group_length: 5 #batch_size  3
  max_epoch: 100
  buffer_size: 1024 #这里应该和抓取的top_k的数量一致 300
  num_sample_points: 1024  #抓取的采样点，这个在抓取网络训练的过程中就已经定死了

learn:
  agent_name: shadow_hand
  test: False
  resume: 0
  save_interval: 200 # check for potential saves every this many iterations
  eval_interval: 10   # multiply by max_episode length
  eval_round: 10
  print_log: True

  # rollout params
  max_iterations: 50000

  # training params
  cliprange: 0.2
  ent_coef: 0
  nsteps: 16
  noptepochs: 8
  nminibatches: 1 # this is per agent
  max_grad_norm: 1
  optim_stepsize: 3.e-4 # 3e-4 is default for single agent training with constant schedule
  schedule: adaptive # could be adaptive or linear or fixed
  desired_kl: 0.008
  gamma: 0.99
  lam: 0.95
  init_noise_std: 0.8

  log_interval: 1
  asymmetric: False